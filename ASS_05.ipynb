{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JatgPdG-o2pL",
        "outputId": "cb21a638-265e-4dcd-a344-775cfdef3902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['the', 'sun', 'rises', 'in', 'the', 'east', 'and', 'sets', 'in', 'the', 'west']\n",
            "Vocab size: 8\n",
            "Sample training pair: (['the', 'sun', 'in', 'the'], 'rises')\n",
            "Epoch 0, Loss = 19.6587\n",
            "Epoch 5, Loss = 19.6587\n",
            "Epoch 10, Loss = 19.6587\n",
            "Epoch 15, Loss = 19.6587\n",
            "the : [0.00112608 0.74159383 0.99206351 0.08542322 0.28640219 0.15411002\n",
            " 0.5929352  0.43046107 0.23301669 0.48032745]\n",
            "in : [0.59054093 0.30682978 0.35826303 0.18887838 0.20505426 0.92342662\n",
            " 0.93151653 0.45124081 0.70979193 0.92289672]\n",
            "sets : [0.91378971 0.97253372 0.16767662 0.53792175 0.90431207 0.98299663\n",
            " 0.36514551 0.45458731 0.07404879 0.54000758]\n",
            "west : [0.11169505 0.2667019  0.61038821 0.28905056 0.29913764 0.08383021\n",
            " 0.4817099  0.2213171  0.80479563 0.75457591]\n",
            "sun : [0.49283255 0.0923667  0.47975027 0.63819327 0.3896817  0.34262669\n",
            " 0.20226906 0.66552097 0.57952104 0.19869312]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# sample text\n",
        "sentences = \"The sun rises in the east and sets in the west\"\n",
        "\n",
        "# remove special characters\n",
        "sentences = re.sub(r'[^a-zA-Z]', ' ', sentences)\n",
        "\n",
        "# lowercase and split into words\n",
        "sentences = sentences.lower()\n",
        "words = sentences.split()\n",
        "\n",
        "print(\"Words:\", words)\n",
        "vocab = set(words)\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 10      # each word will be represented by 10 numbers\n",
        "context_size = 2    # two words before and after as context\n",
        "\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "data = []\n",
        "for i in range(2, len(words) - 2):\n",
        "    context = [words[i-2], words[i-1], words[i+1], words[i+2]]\n",
        "    target = words[i]\n",
        "    data.append((context, target))\n",
        "\n",
        "print(\"Sample training pair:\", data[0])\n",
        "embeddings = np.random.random_sample((vocab_size, embed_dim))\n",
        "def linear(x, theta):\n",
        "    return np.dot(x, theta)        # simple matrix multiplication\n",
        "\n",
        "def log_softmax(x):\n",
        "    x = x - np.max(x)              # for numerical stability\n",
        "    return np.log(np.exp(x) / np.exp(x).sum())\n",
        "learning_rate = 0.01\n",
        "theta = np.random.randn(embed_dim, vocab_size)\n",
        "\n",
        "for epoch in range(20):\n",
        "    total_loss = 0\n",
        "    for context, target in data:\n",
        "        # Average embedding of context words\n",
        "        context_vecs = np.mean([embeddings[word_to_ix[w]] for w in context], axis=0)\n",
        "\n",
        "        # Predict target word\n",
        "        out = linear(context_vecs, theta)\n",
        "        logs = log_softmax(out.reshape(1, -1))\n",
        "\n",
        "        # Simplified “loss”: take negative of target word log prob\n",
        "        target_index = word_to_ix[target]\n",
        "        loss = -logs[0][target_index]\n",
        "        total_loss += loss\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss = {total_loss:.4f}\")\n",
        "for word in list(vocab)[:5]:\n",
        "    print(word, \":\", embeddings[word_to_ix[word]])\n"
      ]
    }
  ]
}